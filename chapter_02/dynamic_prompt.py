import os
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def run_legacy_style(input):
    print("--- æ–¹å¼ä¸€ï¼šä¼ ç»Ÿç›´è§‚å†™æ³• ---")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = init_chat_model(
        os.getenv("MODEL_NAME"),
        temperature=0.5,
        timeout=10,
        max_tokens=1000,
        api_key=os.getenv("API_KEY"),
        model_provider="ollama", # è¿™é‡Œéœ€è¦å®‰è£… langchain-ollama åŒ…
        base_url=os.getenv("BASE_URL"),
    )

    # 2. å®šä¹‰ Prompt æ¨¡æ¿
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æŒ–äº†ä¸‰ä¸ªâ€œå‘â€ï¼š{field} {style} å’Œ {content}
    template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“æ³¨äº {field} é¢†åŸŸçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ã€‚\n"
             "ç¿»è¯‘è¦æ±‚ï¼šå¿…é¡»å¸¦æœ‰ {style} çš„è¯­æ°”ï¼Œå¹¶ä¿æŒä¸“ä¸šæ€§ã€‚"),
        ("user", "{content}")
    ])

    # 3. æ¸²æŸ“ Prompt (å¡«å‘)
    # è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†å˜é‡å¡«å…¥ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ¶ˆæ¯åˆ—è¡¨ (List[Message])
    messages = template.invoke(input)
    
    print(f"[Debug] æ¸²æŸ“åçš„æ¶ˆæ¯: {messages}")

    # 4. è°ƒç”¨æ¨¡å‹
    response = model.invoke(messages)
    
    print(f"âœ… ç»“æœ: {response.content}\n")


def test_legacy_style():
    input_data_1 = {
        "field": "è½¯ä»¶å·¥ç¨‹",
        "style": "å‚²å¨‡ä¸”ç•¥å¸¦è®½åˆº",
        "content": "Using old-school monolithic architecture for a modern microservice problem is clearly an anti-pattern."
    }

    print("--- åœºæ™¯ä¸€ï¼šå‚²å¨‡çš„è½¯ä»¶å·¥ç¨‹å¸ˆ ---")
    print(f"è¾“å…¥æ–‡æœ¬: {input_data_1['content']}")
    
    run_legacy_style(input_data_1)


    print("\n" + "="*40 + "\n")
    
    # --- 6. åŠ¨æ€è¾“å…¥ 2ï¼šå†å²æ–‡å­¦é¢†åŸŸï¼Œä¼˜é›…è¯­æ°” ---
    input_data_2 = {
        "field": "å†å²æ–‡å­¦",
        "style": "ä¼˜é›…ä¸”å……æ»¡å“²ç†",
        "content": "The long river of time eventually reveals the true measure of a man's character."
    }
    
    print("--- åœºæ™¯äºŒï¼šå“²å­¦çš„å†å²å­¦è€… ---")
    print(f"è¾“å…¥æ–‡æœ¬: {input_data_2['content']}")

    run_legacy_style(input_data_2)


from langchain_core.output_parsers import StrOutputParser

def run_lcel_style(input):
    print("--- æ–¹å¼äºŒï¼šLCEL é“¾å¼å†™æ³• ---")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹ (åŒä¸Š)
    model = model = init_chat_model(
        os.getenv("MODEL_NAME"),
        temperature=0.5,
        timeout=10,
        max_tokens=1000,
        api_key=os.getenv("API_KEY"),
        model_provider="ollama", # è¿™é‡Œéœ€è¦å®‰è£… langchain-ollama åŒ…
        base_url=os.getenv("BASE_URL"),
    )

    # 2. å®šä¹‰æ¨¡æ¿ (åŒä¸Š)
    template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“æ³¨äº {field} é¢†åŸŸçš„ç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ã€‚\n"
             "ç¿»è¯‘è¦æ±‚ï¼šå¿…é¡»å¸¦æœ‰ {style} çš„è¯­æ°”ï¼Œå¹¶ä¿æŒä¸“ä¸šæ€§ã€‚"),
        ("user", "{content}")
    ])

    # 3. å®šä¹‰è¾“å‡ºè§£æå™¨ (å¯é€‰)
    # å®ƒèƒ½æŠŠ AI Message å¯¹è±¡ç›´æ¥è½¬æˆçº¯å­—ç¬¦ä¸²ï¼Œçœå»æˆ‘ä»¬æ‰‹åŠ¨å– .content
    parser = StrOutputParser()

    # 4. ğŸ”— ç»„è£…é“¾ (Chain)
    # æ•°æ®æµå‘ï¼šå­—å…¸è¾“å…¥ -> æ¨¡æ¿æ¸²æŸ“ -> æ¨¡å‹æ¨ç† -> ç»“æœè§£æ
    chain = template | model | parser

    # 5. è°ƒç”¨é“¾
    # ç›´æ¥ä¼ å…¥å­—å…¸ï¼ŒLCEL ä¼šè‡ªåŠ¨åŒ¹é…æ¨¡æ¿ä¸­çš„å˜é‡
    result = chain.invoke(input)

    print(f"âœ… ç»“æœ: {result}")

if __name__ == "__main__":
    # å¦‚æœæƒ³è¿è¡Œä¼ ç»Ÿå†™æ³•ï¼Œè¯·å–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š
    # test_legacy_style()

    input_data_1 = {
        "field": "è½¯ä»¶å·¥ç¨‹",
        "style": "å‚²å¨‡ä¸”ç•¥å¸¦è®½åˆº",
        "content": "Using old-school monolithic architecture for a modern microservice problem is clearly an anti-pattern."
    }

    run_lcel_style(input_data_1)