# å¯¼å…¥ç›¸å…³ä¾èµ–
from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, AIMessage, SystemMessage
import os


# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

def test_translation():
    """
    å®æˆ˜ï¼šæµ‹è¯•ä¸€ä¸ªç®€å•çš„ç¿»è¯‘ä»»åŠ¡
    å±•ç¤º LangChain å¦‚ä½•ç»Ÿä¸€æ¨¡å‹è°ƒç”¨æ¥å£
    """
    

    # --- 1. è·å–é…ç½® ---
    model_name = os.getenv("MODEL_NAME")
    api_key = os.getenv("API_KEY")
    base_url = os.getenv("BASE_URL")
    provider = os.getenv("MODEL_PROVIDER", "ollama")

    print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {model_name} ({provider})...")

    # --- 2. åˆå§‹åŒ–æ¨¡å‹ (æ ¸å¿ƒ) ---
    # init_chat_model æ˜¯ä¸€ä¸ªåŒ…è£…å™¨ï¼Œå¯ä»¥åˆå§‹åŒ–ä¸åŒå‚å•†çš„æ¨¡å‹ã€‚è¿™æ˜¯å®˜æ–¹æä¾›çš„ä¸€ä¸ªç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒçš„æ¨¡å‹å‚å•†åˆ—è¡¨ï¼šhttps://docs.langchain.com/oss/python/integrations/chat
    # ä¹Ÿå¯ä»¥ä½¿ç”¨ ChatOpenAI æ¥åˆå§‹åŒ–ï¼Œéœ€è¦å®‰è£… langchain-openai åŒ…
    # è¿™é‡Œä»¥ Ollama ä¸ºä¾‹ï¼Œéœ€è¦å®‰è£… langchain-ollama åŒ…ã€‚ ç­‰åŒäºä½¿ç”¨ ChatOllama åˆå§‹åŒ–
    model = init_chat_model(
        model_name,
        temperature=0.5,
        timeout=10,
        max_tokens=1000,
        api_key=api_key,
        model_provider="ollama", # è¿™é‡Œéœ€è¦å®‰è£… langchain-ollama åŒ…
        base_url=base_url,
    )

    # --- 3. æ„é€ æ¶ˆæ¯ ---
    # LangChain æä¾›äº†æ¶ˆæ¯ç±»ï¼Œç”¨äºè¡¨ç¤ºä¸åŒè§’è‰²çš„æ¶ˆæ¯
    # message å¯¹è±¡æœ‰ä¸‰ä¸ªå±æ€§ï¼š
    # Role - æ¶ˆæ¯ç±»å‹ (e.g. system, user)
    # Content - æ¶ˆæ¯å†…å®¹ (like text, images, audio, documents, etc.)
    # Metadata - å¯é€‰å­—æ®µï¼Œå¦‚å“åº”ä¿¡æ¯ã€æ¶ˆæ¯ IDã€token ä½¿ç”¨æƒ…å†µç­‰
    
    # SystemMessage - ç³»ç»Ÿæ¶ˆæ¯ï¼Œç”¨äºè®¾ç½®æ¨¡å‹çš„ä¸Šä¸‹æ–‡
    system_msg = SystemMessage("ä½ æ˜¯ä¸€ä¸ªç²¾é€šã€ä¿¡è¾¾é›…ã€‘ç¿»è¯‘å‡†åˆ™çš„åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„è¾“å…¥ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶é™„å¸¦ä¸€å¥ç®€çŸ­çš„èµæã€‚")
    
    # HumanMessage - äººç±»æ¶ˆæ¯ï¼Œç”¨äºè®¾ç½®æ¨¡å‹çš„è¾“å…¥
    human_msg = HumanMessage("The only way to do great work is to love what you do.")

    
    # --- 4. å®šä¹‰ Agent/Chain ---
    agent = create_agent(
        model=model,
        system_prompt=system_msg,
    )

    
   
    print(f"ğŸ”„ æ­£åœ¨å‘é€è¯·æ±‚...")

    # --- 5. è°ƒç”¨æ¨¡å‹ (Invoke) ---
    # æ— è®ºåº•å±‚æ˜¯å“ªä¸ªå‚å•†ï¼Œè¿™é‡Œæ°¸è¿œåªç”¨ .invoke()
    messages = [human_msg] # åœ¨create_agentæ—¶å·²ç»åŒ…å«äº†system_msgï¼Œæ‰€ä»¥è¿™é‡Œåªéœ€è¦human_msgï¼Œ å¦‚æœéœ€è¦å¤šä¸ªæ¶ˆæ¯ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
    # è¾“å…¥ç»“æ„æ ‡å‡†åŒ–ä¸º {"messages": [...]}
    response = agent.invoke({"messages": messages})

    # --- 6. æ‰“å°ç»“æœ ---
    # response æ˜¯ä¸€ä¸ª AIMessage å¯¹è±¡ï¼Œ.content æ‰æ˜¯æ–‡æœ¬å†…å®¹
    # å“åº”çš„ç»“æ„ï¼šhttps://docs.langchain.com/oss/python/api_reference/langchain.schema.messages
    # ä¾‹å¦‚ï¼š{'messages': [HumanMessage(content='The only way to do great work is to love what you do.', additional_kwargs={}, response_metadata={}, id='dced4b3e-016a-4f23-8ea2-b6c5c3a95343'), AIMessage(content='ç¿»è¯‘ï¼šåšå‡ºå“è¶Šæˆå°±çš„å”¯ä¸€é€”å¾„æ˜¯çƒ­çˆ±ä½ æ‰€åšçš„äº‹æƒ…ã€‚\n\nèµæï¼šå°†"great work"è¯‘ä¸º"å“è¶Šæˆå°±"æ—¢ä¿ç•™äº†åŸæ„ï¼Œåˆæå‡äº†è¯­è¨€çš„ä¼˜é›…åº¦ï¼Œä½¿æ•´å¥è¯æ›´å…·æ„ŸæŸ“åŠ›ã€‚', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2025-12-09T09:07:18.43724Z', 'done': True, 'done_reason': 'stop', 'total_duration': 36826486958, 'load_duration': 83662500, 'prompt_eval_count': 58, 'prompt_eval_duration': 852940125, 'eval_count': 538, 'eval_duration': 35473802959, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019b025c-e739-7c21-8431-d9d02c8119c3-0', usage_metadata={'input_tokens': 58, 'output_tokens': 538, 'total_tokens': 596})]}
    # response['messages'][-1] æ°¸è¿œæ˜¯ AI çš„æœ€æ–°å›å¤
    ai_message = response['messages'][-1]
   
    print("\n-------- ğŸ“ ç¿»è¯‘ç»“æœ --------")
    print(ai_message.content)
    print("----------------------")

    
    # 7. æŸ¥çœ‹ Token æ¶ˆè€— (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
    # è¿™é‡Œå¯ä»¥ç›‘æ§ Token æ¶ˆè€—ï¼Œç”¨äºæˆæœ¬è®¡ç®—
    if hasattr(ai_message, 'usage_metadata'):
        usage = ai_message.usage_metadata
        print(f"\nğŸ“Š Token æ¶ˆè€—: Input {usage.get('input_tokens')} / Output {usage.get('output_tokens')}")

if __name__ == "__main__":
    test_translation()